{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final-ish models",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TwaitesJosh/Concha/blob/main/Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDauEnd1PXLn"
      },
      "source": [
        "#Report\n",
        "\n",
        "Joshua Twaites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMzrwbKEqLCO"
      },
      "source": [
        "### Disclaimer\n",
        "\n",
        "Most of the code in this report is has been modified for ease of understanding, it may not actually run, luckilly it shouldn't need to. As that is what the github code is for.\n",
        "\n",
        "Also, , I cannot spell, and google colab doesn't have a spell check. I apologise for any typos.\n",
        "\n",
        "All working code can be found on https://github.com/TwaitesJosh/Concha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giOUQmtePeRi"
      },
      "source": [
        "##Summary\n",
        "\n",
        "tl;dr \n",
        "\n",
        "Everything I did in brief. More details below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE4z6B8zPlPV"
      },
      "source": [
        "###Goal\n",
        "Build a classifier that can predict {500k, 1k, 3k , 8k} from features of {2k, 4k, 6k}."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbVaxMPP2VZ"
      },
      "source": [
        "### Data exploration: \n",
        "*\tAbout 700,000 files had a ‘missing’ variable (either ‘**’ or values which were not multiples of 5).\n",
        "*\tMost common class was {2k: 0, 4k:0, 6k:0}, creating a huge imbalance in data.\n",
        "*\tDespite {2k,4k,6k} being integers, they can be viewed as categorical/ordinal as only 22 possible values exist (as only multiples of 5 are acceptable). This means there are only 10,648 possible unique inputs for the classifer.\n",
        "*\tWhen including right/left we have around 4,000,000 examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJBS_yz-QHjp"
      },
      "source": [
        "###Model building: \n",
        "*\tGiven the imbalance, missing data and categorical inputs, this task is particularly suited to K-Nearest Neighbours (KNN), which ignores imbalance, missing data (if done properly) and allows for categorical inputs.\n",
        "*\tKNN is not suited to such large data sets, however, as there are only 10,648 possible unique values for the features we can just build the model on these unique values (using eager memorisation).\n",
        "* I used K=300, with the addendum that I add all people whose distance is equal to the 300th furthest person, as the feature values are discrete, lots of people have the same distances.\n",
        "*\tTo speed up prediction we can precompute all 10,648 inputs and their corresponding outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6NtdCAYRWEg"
      },
      "source": [
        "### Evaluation:\n",
        "* I optimised hyperparameters against R2-score and weighted kappa statistic. I assumed accuracy was not the focus, as it is better to predict {0,0,0,0} than {100,100,100,100} for a correct output of {5,5,5,5}, whereas accuracy rates these two equally. R2-score averaged around 0.70-0.75, which was better than a random forest regressor I used as a comparison.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBKAsV8zRldW"
      },
      "source": [
        "### Optional tasks:\n",
        "*\tThe API was created using flask and uploaded to ec2.aws. The address is:\n",
        "\n",
        "`ec2-18-118-160-157.us-east-2.compute.amazonaws.com`\n",
        "\n",
        "*\tFor a given input x, I found the optimal fourth variable by finding the closest 300 people. I computed the gini-impurity on the data from these 300 with the variables {500k, 1k, 3k, 8k} and found which variable minimised the value gini value. I used this as the extra variable for classification. As before, to ensure I do not have to recompute this every time, I computed this for every 10,648 possible inputs, although as I took a fourth input there are 234,256 possible inputs, representing a large but not too large data base (40Mb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_L2-gosScPO"
      },
      "source": [
        "### Key points/Additions:\n",
        "*\tThe inputs to all of the models are not error checked, if you do not put a variable that is a multiple of 5, or do not put in a list of 3/4 numbers, they will not work. I did not have time to fix this.\n",
        "*\t The API website is not fantastic, I assume you are judging me based on the performance of the model not my web-design skills.\n",
        "*\tAll models in the github simply read the inputs from the pre-computed csv, the actual code to create them is in this report.\n",
        "*\tThe command line interfaces only accept a single input at once. The models can easily handle any size input as they are just pandas merge and lookup functions, but to handle user input, I kept the CLI’s as single input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4MwYAWIme6H"
      },
      "source": [
        "## CLI\n",
        "I have saved everything in github and created a docker image. The docker image represents the CLI.\n",
        "\n",
        "The commands to access it are:\n",
        "\n",
        "\n",
        "` docker pull twaitesjosh/concha-main `\n",
        "\n",
        "\n",
        "` docker run -i twaitesjosh/concha-main `\n",
        "\n",
        "The -i tag is becuase it is interactive.\n",
        "\n",
        "\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcO8zbPIly4Q"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score as r2\n",
        "from sklearn.ensemble import RandomForestClassifier as RFR\n",
        "from sklearn.neighbors import NearestNeighbors as NN\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnmmCablS0gx"
      },
      "source": [
        "## Goal\n",
        "The goal of the take home task was to predict the values of {500k, 1k, 3k, 8k} from inputs of {2k, 4k, 6k}, and to provide a CLI to allow non-tech savvy users to interact with it. Checking with Leslie said that docker containers were acceptable. I doubt non-tech savvy people can use docker, but there you go.\n",
        "\n",
        "\n",
        "Optional goals were to create an API and to suggest a fourth variable to allow the user to improve performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbrqFyXcSyu_"
      },
      "source": [
        "### Take away\n",
        "Classic ML problem using 3 inputs for a multi-class output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCmE3uWZTCjI"
      },
      "source": [
        "## Data exploration\n",
        "\n",
        "The data has 2,900,000-ish rows, with the following columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IOgBiHIlTWK",
        "outputId": "928cc8e2-84f5-4662-8f5c-4678817ae6e2"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "tab = pd.read_csv('/content/drive/My Drive/g.csv', low_memory=False)\n",
        "print(tab.shape)\n",
        "print(tab.columns)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(2944670, 21)\n",
            "Index(['test_date', 'nid', 'L500k', 'L1k', 'L2k', 'L3k', 'L4k', 'L6k', 'L8k',\n",
            "       'R500k', 'R1k', 'R2k', 'R3k', 'R4k', 'R6k', 'R8k', 'gender', 'naics',\n",
            "       'age_group', 'region', 'NAICS_descr'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8j8zlpJT7Vu"
      },
      "source": [
        "As I need to be able to predict the {500k, 1k, 3k, 8k} with just {2k, 4k, 6k} I can drop all of the other rows from my data. While there are surely insights that can be gained from gender and age range, in these problems I don’t have access to that information, so I can ignore it. Additionally, I can treat the left and right values as entirely separate according to the problem sheet.\n",
        "\n",
        "In the data the features have a diverse range of potential values, however checking with Leslie tells me that only the values that are multiples of 5 are actually correct. Due to this constraint in feature values, it is possible to view them as categoric values (or at least ordinal values). As there are three features each with 22 possible values (ranging from -5 to 100), I actually only have 10648 (22^3) possible unique inputs to my model. Obviously, there are more rows than this, meaning feature values repeat, unfortunately their {500k, 1k, 3k, 8k} values do not necessarily repeat. Therefore, it is not possible to have a perfect classification (it rarely is, to be honest)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIeMoWe_mOFH"
      },
      "source": [
        "def clean_data(tab, keep_nans = True):\n",
        "  # Remove all the non-numeric rows\n",
        "  tab_numeric = tab[['L500k', 'L1k', 'L2k', 'L3k', 'L4k', 'L6k', 'L8k', 'R500k', 'R1k', 'R2k', 'R3k', 'R4k', 'R6k', 'R8k']]\n",
        "  # Force everything to either number or NaN\n",
        "  tab_numeric = tab_numeric.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "\n",
        "  YL = tab_numeric[['L500k', 'L1k', 'L2k', 'L3k', 'L4k', 'L6k', 'L8k']].dropna()\n",
        "  YR = tab_numeric[['R500k', 'R1k', 'R2k', 'R3k', 'R4k', 'R6k', 'R8k']].dropna()\n",
        "\n",
        "  # Remove everything that sin't a mutliple of 5 (as this is an error)\n",
        "  YR=YR[(YR%5==0).sum(1) ==7] \n",
        "  YL=YL[(YL%5==0).sum(1) ==7]\n",
        "\n",
        "  # Match names\n",
        "  YL = YL.rename(columns={'L500k': '500k', 'L1k': '1k','L2k': '2k','L3k': '3k','L4k': '4k','L6k': '6k','L8k': '8k'})\n",
        "  YR = YR.rename(columns={'R500k': '500k', 'R1k': '1k','R2k': '2k','R3k': '3k','R4k': '4k','R6k': '6k','R8k': '8k'})\n",
        "  \n",
        "  # Stack on top of each other\n",
        "  Y = pd.concat([YL,YR],ignore_index=True)\n",
        "\n",
        "  # If we want to keep the NA values, repeat the process with them\n",
        "  if keep_nans:\n",
        "    is_NaN = tab_numeric.isnull()\n",
        "    row_has_NaN = is_NaN.any(axis=1)\n",
        "    rows_with_NaN = tab_numeric[row_has_NaN]\n",
        "\n",
        "    Nan_tab = rows_with_NaN[np.sum(np.logical_or(rows_with_NaN%5==0, rows_with_NaN.isna()), 1)==14]\n",
        "    Nan_tabL = Nan_tab[['L500k', 'L1k', 'L2k', 'L3k', 'L4k', 'L6k', 'L8k']]\n",
        "    Nan_tabR = Nan_tab[['R500k', 'R1k', 'R2k', 'R3k', 'R4k', 'R6k', 'R8k']]\n",
        "    Nan_tabL = Nan_tabL.rename(columns={'L500k': '500k', 'L1k': '1k','L2k': '2k','L3k': '3k','L4k': '4k','L6k': '6k','L8k': '8k'})\n",
        "    Nan_tabR = Nan_tabR.rename(columns={'R500k': '500k', 'R1k': '1k','R2k': '2k','R3k': '3k','R4k': '4k','R6k': '6k','R8k': '8k'})\n",
        "    Y_Nan = pd.concat([Nan_tabL, Nan_tabR],ignore_index=True)\n",
        "\n",
        "    Y = pd.concat([Y,Y_Nan],ignore_index=True)\n",
        "  \n",
        "  return Y"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCzBiUYVU_Cd"
      },
      "source": [
        "Investigating the class balances shows that by far the majority class is {500k:0, 1k:0, 3k:0, 8k:0}, e.g. no hearing problems at all, which is unsurprising."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlb3C0BWqMEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e21dafd-94e0-401d-b53f-ef91675f9a21"
      },
      "source": [
        "Y = clean_data(tab, keep_nans = False)\n",
        "X = Y[['2k','4k','6k']]\n",
        "np.sum(np.all(Y[['2k','4k','6k']] == [0,0,0], 1))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83004"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu4ajFCWVkdV"
      },
      "source": [
        "Around 1,500,000 rows had some data missing, representing about 30%. Obviously, this needs to be dealt with, however removing it entirely means throwing away a lot of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOT-fXzqaohq",
        "outputId": "3a2a7d67-2c07-4e69-f9ac-43d1f30847a8"
      },
      "source": [
        "Y_nan = clean_data(tab, keep_nans = True)\n",
        "print(Y_nan.shape[0] - Y.shape[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1576562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CqR9X47WCQO"
      },
      "source": [
        "### Take away \n",
        "I have 3 categorical (ordinal) features and 10648 possible unique different inputs; the data set is hugely imbalanced, and I have a lot of missing data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4XClThuWLmq"
      },
      "source": [
        "possible_X_values = np.array(list(itertools.product(5*np.arange(-1, 21), repeat = 3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZCvxfhmWTsJ"
      },
      "source": [
        "## Model:\n",
        "\n",
        "Because of the missing data, class imbalance and the limited potential inputs to the classifier (10648 potential inputs) I choose a K-Nearest neighbour’s approach. Where for a given input X, the predicted classes were the average (mean) of the closest 300 people. When I take the mean, I can ignore the missing value in rows, so I get to use the non-missing data from rows that do contain missing data, which partially mitigates the missing data issue.\n",
        "\n",
        "\n",
        "As the feature values were discrete, the distances were also discrete, and therefore in picking the closest 300 was difficult. So instead, I iterated through the potential differences (low to high) until I had at least 300 neighbours. So, if I have 299 rows who have a distance of less than 5 from input X, and 4000 people are 5.2 away. I will end up examining 4299 neighbours not just 300.\n",
        "\n",
        "\n",
        "Class imbalance doesn’t really impact KNN (it does, but not in this case) so I can ignore it.\n",
        "\n",
        "\n",
        "\n",
        "One of the issues with KNN is it is very slow, both in the training and testing. However, I can mitigate this by using the fact that there are only 10648 possible inputs to the model, so I can train only on these unique values and apply my results to the entire data set. This allows me to train the entire model in under 30 seconds. A similar approach can be taken for the prediction step. As there are only 10648 possible inputs to the model, there are only 10648 possible outputs to the model, I can pre-compute these and store them as a data frame. This technique is known as eager memoisaton, it is contextually similar to dynamic programming but done in reverse, so all the memorisation is done first. When I predict a value, I just have to look it up in the data frame, which is much faster than a KNN and takes much less storage space. This is why all of the model.py’s in the github are just pandas look-up functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLzR1pSQeBPB"
      },
      "source": [
        "class model:\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\t# All possible X_values as a set of tuples\n",
        "\t\tself.possible_X_values = np.array(list(itertools.product(5 * np.arange(-1, 21), repeat=3)))\n",
        "\t\tself.set_of_all_possible_x = set([tuple(x) for x in self.possible_X_values])\n",
        "\t\t\n",
        "\t\tself.group_indexes = None\n",
        "\t\tself.nn = None\n",
        "\t\tself.memorised_data = None\n",
        "\n",
        "\tdef fit(self, X_train, y_train):\n",
        "    # Collapse y-train into unique values of 2k, 4k, 6k and create a set of where they occour\n",
        "\t\tgroups = y_train.groupby(['2k', '4k', '6k'])\n",
        "\t\tself.group_indexes = groups.indices\n",
        "\t\tset_of_all_X_train = set(self.group_indexes)\n",
        "\n",
        "    # Identify the unique values of 2k, 4k, 6k in X and create a KNN classifer on them\n",
        "\t\tself.x_unique = X_train.groupby(['2k', '4k', '6k']).size().reset_index().rename(columns={0: 'count'})\n",
        "\t\tself.nn = NN(metric='minkowski', p=3)\n",
        "\t\tself.nn.fit(self.x_unique[['2k', '4k', '6k']])\n",
        "\t\t\n",
        "\t\t# make sure group_indices has all possible x values\n",
        "\t\tunused_keys = self.set_of_all_possible_x.difference(set_of_all_X_train)\n",
        "\t\tfor k in unused_keys:\n",
        "\t\t\tself.group_indexes[k] = []\n",
        "\n",
        "\t\tself.memorised_data = self.eager_memoisation(y_train)\n",
        "\n",
        "\n",
        "\tdef get_Y_train_standard(self, i, y_train):\n",
        "   # Find all neigbors within 50 (an arbitrarly large value)\n",
        "\t\tten_n = self.nn.radius_neighbors(i, 50)\n",
        "   # Find the unique distances in sorted order, makes use of the fact the unique also sorts that data\n",
        "\t\tuniq = np.unique((ten_n[0][0])) \n",
        "\t\ttotal = 0\n",
        "\t\tindexes = []\n",
        "    # for each distance, find the people in Y-train who re that far away\n",
        "\t\tfor d in uniq:\n",
        "      # Add indexes of people who are distance d away from input row\n",
        "\t\t\ttrain_loc = ten_n[1][0][ten_n[0][0] == d]\n",
        "      # Keep total amount of people who are being used as neigbors\n",
        "\t\t\ttotal += np.sum(self.x_unique['count'].values[train_loc])\n",
        "\t\t\tindexes = np.concatenate((indexes, train_loc))\n",
        "     # if 300 or more neighbors, break\n",
        "\t\t\tif total > 300:\n",
        "\t\t\t\tbreak\n",
        "\t\tm = indexes\n",
        "\t\tl = []\n",
        "    # Find all people in Y_train who are within distance of row I\n",
        "\t\tfor j in m:\n",
        "\t\t\tl = np.concatenate((self.group_indexes[tuple(self.x_unique.iloc[int(j)].values[:3])], l))\n",
        "\t\tl = np.array(l, dtype=np.int)\n",
        "    # Table of all people within distance of row I\n",
        "\t\tmode = y_train.iloc[l][['500k', '1k', '3k', '8k']]\n",
        "\t\treturn mode\n",
        "\n",
        "\tdef get_Y_lab(self, i, y_train):\n",
        "   # Just get the mean of the table and make sure it is a multiple of 5\n",
        "\t\ttab = self.get_Y_train_standard(i, y_train)\n",
        "\t\treturn np.round(tab.mean(0) / 5) * 5\n",
        "\n",
        "\tdef eager_memoisation(self, y_train):\n",
        "   # for all unique values of x, create corresponding y_values\n",
        "\t\ty_unique_labs = pd.DataFrame(self.possible_X_values, columns=['2k', '4k', '6k']).apply(\n",
        "\t\t\tlambda x: self.get_Y_lab([x], y_train), 1)\n",
        "\t\ty_unique_labs = pd.DataFrame(y_unique_labs.values.tolist(), index=y_unique_labs.index, columns=['500k', '1k', '3k', '8k']) \n",
        "\t\tlabeled_x_data = pd.DataFrame(self.possible_X_values, columns=['2k', '4k', '6k']).join(y_unique_labs)\n",
        "\t\treturn labeled_x_data\n",
        "\n",
        "\tdef predict(self, X_test):\n",
        "   # Look up X_test in the memorised data\n",
        "   # Merge does tend to unsort the data, so the resorting by the index is required\n",
        "\t\ty_pred = X_test.reset_index().merge(self.memorised_data, left_on=['2k', '4k', '6k'], right_on=['2k', '4k', '6k'], how='left').sort_index()\n",
        "\t\ty_pred = y_pred.drop(columns=['index'])\n",
        "\t\treturn y_pred[['500k', '1k','2k','3k','4k','6k', '8k']]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhq61HiibHl4"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "### Metrics\n",
        "The main metric I optimised in this work was R_2 score. I operated under the assumption that a prediction of {5,5,5,5} should be considered closer to a target of {0,0,0,0} than {100,100,100,100}, therefore accuracy was not a good metric to use. R_2 allowed for this notion of closeness along with being very quick to compute and allowing for the multi-output of the model.\n",
        "I also utilised weighted Kappa, which is a metric that measures the degree of disagreement between the two raters (the predictor and the truth, in this case), the higher the disagreement the higher the weight (so prediciting {5,5,5,5} is 10 times worse than predicting {50,50,50,50} if the truth is {0,0,0,0}).\n",
        "\n",
        "\n",
        "### Optimisation:\n",
        "In general, changing the hyperparameters of the model has a neglible influence on the performance of the model, with the main source of error being that there is not a direct mapping of input to output, as the same input can have different outputs in the training data. Although it was found that adding the missing data improved the performance by about 0.02-0.05. Changing the metric and the value of K (number of neighbors) has limited influence on CV score.\n",
        "\n",
        "\n",
        "### Comparison:\n",
        "I created a very simple Random Forest Regressor to act as a ‘standard approach’ as RF’s are also relatively immune to class imbalance and perform well in almost every circumstance. This obtained an R_2 score of 0.7 without any optimisation. This was the standard that I compared my classifier to.\n",
        "\n",
        "\n",
        "### Overfitting:\n",
        "In general, KNN can be safe from overfitting as long as you use a large enough value for K, 300 is quite large. Additionally, optimising over cross-validated performance helps mitigate the impact of overfitting. I used 10-fold CV, making sure that the left and right data from people were always in the same groups to mitigate any intra-person impacts on the data. As I didn't have time to study the indepth impacts of this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2ndHpxScPD-"
      },
      "source": [
        "## Command line interface\n",
        "\n",
        "I created docker repositories for the task, so in theory you can just pull these and run them.\n",
        "\n",
        "\n",
        "The commands to access it are:\n",
        "\n",
        "\n",
        "` docker pull twaitesjosh/concha-main `\n",
        "\n",
        "\n",
        "` docker run -i twaitesjosh/concha-main `\n",
        "\n",
        "\n",
        "It is worth noting that you require the -i tag on docker run, this is becuase it is interactive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpFJXldzZKto"
      },
      "source": [
        "m = model()\n",
        "m.fit(X_train, y_train)\n",
        "# Tab is the look-up table that the github models read\n",
        "tab = m.memorised_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSXPTbppc74J"
      },
      "source": [
        "## Optional task 1:\n",
        "Create an API\n",
        "\n",
        "The API is created using flask. I am hosting it on an AWS ec2 server.\n",
        "\n",
        "`ec2-18-118-160-157.us-east-2.compute.amazonaws.com`\n",
        "\n",
        "This is an example curl request:\n",
        "\n",
        "\n",
        "` curl -X POST ec2-18-118-160-157.us-east-2.compute.amazonaws.com:80/predict -H \"Content-Type: application/json\" -d \"[5,30,5]\" `\n",
        "\n",
        "\n",
        "It is worth noting, that linux and windows get really ansty if you mix up ' and \" in a curl request. I lost 6 hours of my life fixing that issue.\n",
        "\n",
        "The website itself is not great, obviously I am not a web designer. The functionality of the API is identical to the Concha-main. It won't take multiple inputs and doesn't error check the inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjtEaP7Ze0Ow"
      },
      "source": [
        "## Optional task 2:\n",
        "Allow users to add a fourth variable.\n",
        "\n",
        "\n",
        "With this task I once again made use of eager memoisation. For each of the 10648 possible inputs, I identified the K-nearest neighbours, as above. I then computed the GINI impurity on the features {500k, 1k, 3k, 8k} to see which of the one of the potential new features best split the data. The feature with the lowest GINI was select as the ‘extra’ variable for the input. Then for each potential value of that input I computed the closet 300 neighbours and identified the mean value.\n",
        "\n",
        "\n",
        "As I am now using 4 variables my potential unique values number are 234,256 which is a little large for a csv, however it only takes up about 30Mb, so it is fine to be mounted on a server. The computation of the final lookup table took around 1-hour.\n",
        "\n",
        "\n",
        "The container that runs this is avaiable through docker, named\n",
        "\n",
        "\n",
        "The commands to access it are:\n",
        "\n",
        "\n",
        "` docker pull twaitesjosh/concha-opt2 `\n",
        "\n",
        "\n",
        "` docker run -i twaitesjosh/concha-opt2 `\n",
        "\n",
        "\n",
        "It is worth noting that you require the -i tag on docker run, this is becuase it is interactive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jiuy_OsK_zwt"
      },
      "source": [
        "# Computes the GINI coeff of a column\n",
        "def gini_coefficient(x):\n",
        "\tx = x.values\n",
        "\tdiffsum = 0\n",
        "\tfor i, xi in enumerate(x[:-1], 1):\n",
        "\t\tdiffsum += np.sum(np.abs(xi - x[i:]))\n",
        "\treturn diffsum / (len(x)**2 * np.mean(x))\n",
        "\n",
        "# This is added to the model class, allows it to find the minimum GINI value\n",
        "def find_fourth_variable(self, i, y_train):\n",
        "  tab = self.get_Y_train_standard(i, y_train)\n",
        "  return tab.dropna().apply(gini_coefficient, 0).idxmin()\n",
        "  \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en5O_6WyQPhq"
      },
      "source": [
        "m = model()\n",
        "m.fit(X_train, y_train)\n",
        "# Create the memeorised datm then compute the optimal extra value\n",
        "tab_4th_vari = m.memorised_data[['2k','4k','6k']].apply(lambda x: m.find_fourth_variable([x], y_train), 1)\n",
        "tab = m.memorised_data[['2k','4k','6k']]\n",
        "tab['extra row'] = tab_4th_vari\n",
        "\n",
        "# Create all possible unique combinations of {2k,4k,6k and the optional variable}\n",
        "possible_X_values = np.array(list(itertools.product(5*np.arange(-1, 21), repeat = 4)))\n",
        "tab_4 = pd.DataFrame(possible_X_values, columns = ['2k', '4k', '6k', 'extra row value'])\n",
        "tab_4['extra row'] = np.repeat(tab['extra row'].values, 22)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XgGfKIuhOzt"
      },
      "source": [
        "I ran each extra row value {'500k, '1k', '3k','8k'} seperately, so I didn't have to redefine the KNN algorithm each time. I then merged all of the tables together, to create a csv with all possible input values which I can use as a lookup table in the github code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxWpOrm4E5FP"
      },
      "source": [
        "tab_500k = tab[tab['extra row']=='500k']\n",
        "variable = '500k'\n",
        "groups = y_train.groupby(['2k', '4k', '6k',variable])\n",
        "group_indexes =  groups.indices\n",
        "    \n",
        "x_unique = y_train.groupby(['2k','4k', '6k', variable]).size().reset_index().rename(columns={0:'count'})\n",
        "nn = NN(metric='minkowski', p=3)\n",
        "nn.fit(x_unique[['2k','4k', '6k', variable]])\n",
        "  \n",
        "\n",
        "store = tab_500k.apply(lambda x: find_4th_metric(x[['2k', '4k', '6k', 'extra row value']]), 1)\n",
        "\n",
        "def find_4th_metric(row):\n",
        "\n",
        "  ten_n = nn.radius_neighbors([row], 80)\n",
        "  \n",
        "  uniq = np.unique((ten_n[0][0]))\n",
        "  total=0\n",
        "  indexes = []\n",
        "  for i in uniq:\n",
        "    train_loc = ten_n[1][0][ten_n[0][0]==i]\n",
        "    total += np.sum(x_unique['count'].values[train_loc])\n",
        "    indexes = np.concatenate((indexes, train_loc)) \n",
        "    if total>300:\n",
        "      break\n",
        "  m = indexes\n",
        "  l = []\n",
        "  for j in m:\n",
        "    l = np.concatenate((group_indexes[tuple(x_unique.iloc[int(j)].values[:4])], l))\n",
        "  l = np.array(l, dtype = np.int)\n",
        "\n",
        "  mode = y_train.iloc[l][['500k','1k', '3k','8k']]\n",
        "\n",
        "  mean =  np.round(mode.mean(0)/5)*5\n",
        "  return mean\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGNwdPFxMZ9g"
      },
      "source": [
        "# Save/load table for each of the possible optional variables\n",
        "tab_8k = pd.read_csv('8k.csv')\n",
        "tab_1k = pd.read_csv('1k.csv')\n",
        "tab_3k = pd.read_csv('3k.csv')\n",
        "tab_500k = pd.read_csv('500k.csv')\n",
        "\n",
        "tab_8k_complete = tr[tr['extra row']=='8k'].merge(tab_8k, left_index=True, right_on= 'Unnamed: 0')\n",
        "tab_1k_complete = tr[tr['extra row']=='1k'].merge(tab_1k, left_index=True, right_on= 'Unnamed: 0')\n",
        "tab_3k_complete = tr[tr['extra row']=='3k'].merge(tab_3k, left_index=True, right_on= 'Unnamed: 0')\n",
        "tab_500k_complete = tr[tr['extra row']=='500k'].merge(tab_500k, left_index=True, right_on= 'Unnamed: 0')\n",
        "\n",
        "# Merge them all together, and final csv is the csv that the classifer can load\n",
        "t1 = pd.concat([tab_1k_complete,tab_3k_complete], 0)\n",
        "t2 = pd.concat([tab_500k_complete, tab_8k_complete], 0)\n",
        "t3 = pd.concat([t1, t2], 0)\n",
        "t3.to_csv('final.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}